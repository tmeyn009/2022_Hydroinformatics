{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:0.2px solid black\"> </hr>\n",
    "\n",
    "<figure>\n",
    "  <IMG SRC=\"img/ntnu_logo.png\" WIDTH=250 ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "**<ins>Course:</ins>** TVM4174 - Hydroinformatics for Smart Water Systems\n",
    "\n",
    "# <ins>Example:</ins> Data Mining - Clustering and Classification Methods and Tools\n",
    "    \n",
    "*Developed by David B. Steffelbauer*\n",
    "\n",
    "<hr style=\"border:0.2px solid black\"> </hr>\n",
    "\n",
    "    \n",
    "## Data Mining Example: Smart Meter Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful packages and predefined functions\n",
    "\n",
    "First, we will import some packages that will be useful for our task to analyse the smart meter data. You already know most of the packages, nevertheless, here is a list of them including links to their documentation:\n",
    "- [numpy](https://numpy.org/): a library adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays \n",
    "- [matplotlib](https://matplotlib.org/): a library for creating static, animated, and interactive visualizations in Python\n",
    "- [pandas](https://pandas.pydata.org/): Python data analysis library\n",
    "- [seaborn](https://seaborn.pydata.org/): statistical data analysis library, which provides a high-level interface for drawing attractive and informative statistical graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set_style('darkgrid')\n",
    "colors = sns.color_palette('magma', 3)\n",
    "from cycler import cycler\n",
    "mpl.rcParams['axes.prop_cycle'] = cycler(color=colors)\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_patterns(patterns, ax=None, color=None, legend=False, alpha=0.5):\n",
    "    \"\"\"Plotting routine for patterns\n",
    "    \n",
    "    :param pattern: Patterns to be plotted\n",
    "    :param ax: matplotlib axes object\n",
    "    :param color: color of the patterns\n",
    "    :param legend: if True, a legend is printed\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    time = patterns.index\n",
    "    y = patterns.values\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "        \n",
    "    ax.plot(time, y, color=color, alpha=alpha)\n",
    "#     ax.plot(time, y.mean(axis=1).values, color='k', linestyle='--', linewidth=2)\n",
    "    if legend:\n",
    "        plt.legend(patterns.columns, fontsize=16)\n",
    "    ax.set_xlim((time[0], time[-1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(data, labels=None, centers=None, cs='w'):\n",
    "    \"\"\"Plotting routine for 2-dimensional figures containing clusters with their centers and \n",
    "    colored datapoints according to their labels\n",
    "    \n",
    "    :param data: Clustering input data, either numpy array or pandas DataFrame,\n",
    "        columns are features, rows are samples\n",
    "    :param labels: Clustering output data, either list, numpy vector or pandas Series,\n",
    "        with the same length as the rows of data\n",
    "    :param centers: Array containing the cluster centers' x and y coordinate\n",
    "    :param cs: face color of the datapoints, when labels are not specified\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        data = data.values\n",
    "    if labels is not None:\n",
    "        n_clusters = len(set(labels))\n",
    "        colors = sns.color_palette('plasma', n_clusters)\n",
    "        cs = [colors[x] for x in labels]\n",
    "\n",
    "    plt.scatter(data[:, 0], data[:, 1], s=100, alpha=0.7, c=cs, edgecolors='k')\n",
    "\n",
    "    if centers is not None:\n",
    "        for center in centers:\n",
    "            plt.scatter(*center, marker='s', s=100, edgecolor='k', linewidths=4, c='None')\n",
    "\n",
    "    plt.xlabel(r'$x_1$', fontsize=18)\n",
    "    plt.ylabel(r'$x_2$', fontsize=18)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.axis('equal')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "\n",
    "<img src=\"img/Problem_Statement.png\" width=800 height=800/>\n",
    "\n",
    "\n",
    "The CEO of L-town has sleepless nights, because he is worried about the unpredictability of the water demand in his water distribution system (WDS). Although, forecasting tools using time series analysis algorithms were developed for his WDS, the forecasts are still not sufficient for him. He sees a lot of potential in leak detection, optimal pump scheduling and pressure management, once, the variability in demand in the system is better known. \n",
    "Especially worrysome are the high peaks during the morning, which might drive the WDS to its limits. Are these peaks caused by single users, or is it generated by simultaneously water usage over a big group of  similar users?\n",
    "\n",
    "Inspired by news articles on how Big-data is transforming the water industry, he decides to install Smart Water Meters (SWM) to gain insights into the water use behaviour of his customers. SWM are devices that are capable of measuring and transmitting customer's water consumption in (near) real-time to the Water Utility (WU).\n",
    "\n",
    "However, although the SWM meters provide a lot of data to the WU, the WU staff still struggles to extract relevant knowledge from the huge amount of data. This is where data mining enters the stage..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "The WU was already capable of extracting demand patterns from the data for each SWM. Demand patterns represent the average water use of a property over the course of a day. \n",
    "\n",
    "The demand pattern data is stored in a CSV file in the `data` folder with the filename `demand_pattern.csv`. \n",
    "\n",
    "We use pandas [`read_csv`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) function to load the data from the textfile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'data/demand_pattern.csv'\n",
    "pattern = pd.read_csv(filename, index_col=0, parse_dates=[0], header=0)\n",
    "\n",
    "pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of an index, which is a datetime time object, and columns with the SWM name. \n",
    "\n",
    "We can plot all the data in a 3d plot, to visualize all the data at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x = np.arange(len(pattern.index))\n",
    "y = np.arange(pattern.shape[1])\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = pattern.values\n",
    "Z = Z.T\n",
    "\n",
    "t_format = [x.strftime('%H:%M') for x in pattern.index]\n",
    "\n",
    "colors = sns.color_palette('magma_r', Z.shape[0])\n",
    "for ii in range(Z.shape[0]):\n",
    "    ax.plot(X[ii, :], Y[ii, :], Z[ii, :], color=colors[ii])\n",
    "\n",
    "plt.xlim((x[0], x[-1]))\n",
    "plt.ylim((y[0], y[-1]))\n",
    "\n",
    "ax.xaxis.set_ticks(x[::12 * 3])\n",
    "ax.xaxis.set_ticklabels(t_format[::12 * 3])\n",
    "\n",
    "ax.yaxis.set_ticks(y[::15])\n",
    "ax.yaxis.set_ticklabels(pattern.columns[::15])\n",
    "\n",
    "plt.xticks(rotation=-45, fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "ax.zaxis.set_tick_params(labelsize=14)\n",
    "\n",
    "ax.view_init(20, -120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can pick out certain sensor numbers and compare the data for those SWM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = ['n3', 'n7', 'n8']\n",
    "\n",
    "plot_patterns(pattern[sensors], legend=True, alpha=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "K-means clustering is \"isotropic\" in all directions of space and therefore tends to produce more or less round (rather than elongated) clusters. In this situation leaving variances unequal is equivalent to putting more weight on variables with smaller variance.\n",
    "\n",
    "Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.\n",
    "\n",
    "In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.\n",
    "\n",
    "$$ \\mathbf{x}^{\\prime} = \\frac{\\mathbf{x} - \\overline{\\mathbf{x}}}{\\sigma_\\mathbf{x}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(df):\n",
    "\n",
    "    return (df - df.mean()) / df.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's standardize the data and compare it to the original..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pattern = standardize(pattern)\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(12, 3))\n",
    "\n",
    "pattern[sensors].plot(ax=axs[0])\n",
    "axs[0].set_title('Original Data')\n",
    "\n",
    "s_pattern[sensors].plot(ax=axs[1])\n",
    "axs[1].set_title('Standardized Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some `scikit-learn` basics\n",
    "\n",
    "Now it's time to start to get our hands on the  [`scikit-learn`](https://scikit-learn.org/stable/) package, which is a an open source machine learning library that supports **supervised** and **unsupervised** learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities. \n",
    "\n",
    "Scikit-learn provides dozens of built-in machine learning algorithms and models, called estimators. Each estimator can be fitted to some data using its fit method.\n",
    "\n",
    "The fit method generally accepts 2 inputs:\n",
    "\n",
    "- The samples matrix (or design matrix) $X$. The size of $X$ is typically (n_samples, n_features), which means that samples are represented as rows and features are represented as columns.\n",
    "\n",
    "- The target values $y$ which are real numbers for regression tasks, or integers for classification (or any other discrete set of values). For unsupervized learning tasks, $y$ does not need to be specified. $y$ is usually 1d array where the $i$-th entry corresponds to the target of the $i$-th sample (row) of $X$.\n",
    "\n",
    "Both $X$ and $y$ are usually expected to be numpy arrays or equivalent array-like data types (e.g. `pandas` DataFrames), though some estimators work with other formats such as sparse matrices.\n",
    "\n",
    "Once the estimator is fitted, it can be used for predicting target values of new data (e.g. for classification). \n",
    "\n",
    "For further information, have a look at the [\"An introduction to machine learning with scikit-learn\"](https://scikit-learn.org/stable/tutorial/basic/tutorial.html).\n",
    "\n",
    "Here is also a [link to a Python Cheat Sheet for `scikit-learn`](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature reduction through Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the pattern data, again, to see, how it fits into the data table structure that we discussed previously (e.g. features, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pattern.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "dimensions = 2\n",
    "\n",
    "col_names = [f'x{i}' for i in range(1, dimensions+1)]\n",
    "\n",
    "pca = PCA(dimensions).fit(s_pattern.T)  # PCA algorithm to reduce data to 2 dimensions\n",
    "X = pca.transform(s_pattern.T)  # Map the data into the 2 dimensional space\n",
    "\n",
    "X = pd.DataFrame(X, columns=col_names, index=s_pattern.columns)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *k*-Means Clustering with `scikit-learn`\n",
    "\n",
    "<img src=\"img/kmeans_scheme.png\" width=800 height=800/>\n",
    "\n",
    "From now on, we will focus on the *k*-Means algorithm in `scikit-learn`. First, we import the `KMeans` class. Here is a [link](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) to the API documentation of `Kmeans`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like already introduced in the previous notebooks, we can also have a quick look at the documentation with the `help` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "help(KMeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first input parameter, when we call the KMeans class is the number of clusters $k$ (`n_clusters` keyword). \n",
    "\n",
    "After the Kmeans algorithm is initialized, we use the fit routine to apply it on the data $X$. After that, we can extract the cluster centers and the labels from the attributes of the Kmeans class (attribute `clusters_centers_` and `labels_`, respectively).\n",
    "Furthermore, the `intertia_` attribute gives us the sum of squared distances of the samples to their closest cluster center ($WCSS$ - *within cluster sum of squares**************************************************************************************), which is the objective function of the kMeans problem that we want to minimize\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{k} \\sum_{\\mathbf{p} \\in \\mathbf{S}_i} || \\mathbf{p} - \\mathbf{\\mu}_i ||^2 \\ ,\n",
    "$$\n",
    "where $\\mathbf{S} = \\{S_1, S_2, \\ldots, S_k\\}$ is the partitioning of the samples $\\mathbf{p}$ in $k$ clusters, and $\\mu_i$ is the  centroid (center of mass) of the cluster $S_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "k = 2\n",
    "\n",
    "km = KMeans(k).fit(X)\n",
    "\n",
    "labels = km.labels_\n",
    "centers = km.cluster_centers_\n",
    "wcss = km.inertia_\n",
    "\n",
    "plot_clusters(X, labels=labels, centers=centers)\n",
    "\n",
    "print(f'Number of clusters = {k} => WCSS = {wcss:.2f}')\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    colors = sns.color_palette('magma', k)\n",
    "    fig = plt.figure(figsize=(12,k*4))\n",
    "    for ii in range(k):\n",
    "        idx = s_pattern.columns[labels == ii]\n",
    "        subset = s_pattern[idx]\n",
    "        ax = fig.add_subplot(k + 1, 1, ii + 1)\n",
    "        plot_patterns(subset, ax=ax, color=colors[ii])\n",
    "        plt.title(f'Number of Patterns={len(idx)}', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ideal number of clusters - Elbow method\n",
    "\n",
    "How should we decide, what is the optimal number of clusters present in the data, or what value should we chose for $k$, respectively? \n",
    "\n",
    "One possibility is visual inspection, but that is highly subjective. Furthermore, often times the data you’ll be working with will have multiple dimensions making it difficult to visual. As a consequence, the optimum number of clusters $k$ is no longer obvious. Fortunately, we have a way of determining this mathematically, the elbow method...\n",
    "\n",
    "To decide on the optimal number of clusters, we train multiple models using a different $k$ values and storie the value of the WCSS (`intertia_` property) every time.\n",
    "\n",
    "Of course, this value gets smaller, the more cluster centers are in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "c_numbers = np.arange(2, 16)\n",
    "\n",
    "for k in c_numbers:\n",
    "    \n",
    "    km = KMeans(n_clusters=k).fit(X)\n",
    "    \n",
    "    wcss.append(km.inertia_)\n",
    "    \n",
    "    print(f'{k} Clusters -> WCSS={km.inertia_}')\n",
    "\n",
    "wcss = pd.Series(wcss, index=c_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss.plot(marker='o', color='k', linewidth=2)\n",
    "plt.xlabel('k', fontsize=18);\n",
    "plt.ylabel('WCSS', fontsize=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pattern.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index[labels==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.to_csv('data/labelled_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
